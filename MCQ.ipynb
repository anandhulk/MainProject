{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KcHRQnLHC9U_wonK0gX-_KY_hqQA5J8p",
      "authorship_tag": "ABX9TyMTgzIS+FpZtElf2LPU5ynZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandhulk/MainProject/blob/main/MCQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozv_NRSPTmUK",
        "outputId": "3009cf57-539b-4170-c322-276cad264f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    TokenClassificationPipeline,\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "from transformers.pipelines import AggregationStrategy\n",
        "import numpy as np\n",
        "\n",
        "# Define keyphrase extraction pipeline\n",
        "class KeyphraseExtractionPipeline(TokenClassificationPipeline):\n",
        "    def __init__(self, model, *args, **kwargs):\n",
        "        super().__init__(\n",
        "            model=AutoModelForTokenClassification.from_pretrained(model),\n",
        "            tokenizer=AutoTokenizer.from_pretrained(model),\n",
        "            *args,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def postprocess(self, model_outputs):\n",
        "        results = super().postprocess(\n",
        "            all_outputs=model_outputs,\n",
        "            aggregation_strategy=AggregationStrategy.FIRST,\n",
        "        )\n",
        "        return np.unique([result.get(\"word\").strip() for result in results])\n",
        "        # return results\n"
      ],
      "metadata": {
        "id": "hwW9Mk6XT3ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pipeline\n",
        "model_name = \"ml6team/keyphrase-extraction-distilbert-inspec\"\n",
        "extractor = KeyphraseExtractionPipeline(model=model_name)\n"
      ],
      "metadata": {
        "id": "6hjKWAY5VW6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extractor.save_pretrained(\"/content/drive/MyDrive/keyphrase-extraction-distilbert-inspec\")"
      ],
      "metadata": {
        "id": "F1iMA5vHwtrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = KeyphraseExtractionPipeline(model=\"/content/drive/MyDrive/keyphrase-extraction-distilbert-inspec\")"
      ],
      "metadata": {
        "id": "cAP_tTu3xRbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "text = \"\"\"\n",
        "Keyphrase extraction is a technique in text analysis where you extract the\n",
        "important keyphrases from a document. Thanks to these keyphrases humans can\n",
        "understand the content of a text very quickly and easily without reading it\n",
        "completely. Keyphrase extraction was first done primarily by human annotators,\n",
        "who read the text in detail and then wrote down the most important keyphrases.\n",
        "The disadvantage is that if you work with a lot of documents, this process\n",
        "can take a lot of time. \n",
        "\n",
        "Here is where Artificial Intelligence comes in. Currently, classical machine\n",
        "learning methods, that use statistical and linguistic features, are widely used\n",
        "for the extraction process. Now with deep learning, it is possible to capture\n",
        "the semantic meaning of a text even better than these classical methods.\n",
        "Classical methods look at the frequency, occurrence and order of words\n",
        "in the text, whereas these neural approaches can capture long-term\n",
        "semantic dependencies and context of words in a text.\n",
        "\"\"\".replace(\"\\n\", \" \")\n",
        "\n",
        "keyphrases = model2(text)\n",
        "\n",
        "print(keyphrases)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoLn8KITVcDU",
        "outputId": "19db2958-ea16-49d5-d304-ce8e0afe99c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial intelligence' 'classical machine learning' 'deep learning'\n",
            " 'keyphrase extraction' 'linguistic features' 'statistical'\n",
            " 'text analysis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Set the data directory to a permanent location on your computer\n",
        "nltk.data.path.append('/content/drive/MyDrive/nltk')\n",
        "\n",
        "# Download the 'punkt' package\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HiLF5LXpSkW",
        "outputId": "c9f23798-2b40-449b-d01a-c50a80bec2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.data.path.append('/content/drive/MyDrive/nltk')"
      ],
      "metadata": {
        "id": "WDlkiNcFpxAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # download the punkt package which contains pre-trained models for sentence tokenizationfrom nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZf2NgqnYNEp",
        "outputId": "8fa7304a-e35c-4511-c328-531e5b30ab97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_oF_qruYYSa",
        "outputId": "c4b73431-6861-45ef-f105-7097f7742af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' keyphrase extraction is a technique in text analysis where you extract the important keyphrases from a document.', 'thanks to these keyphrases humans can understand the content of a text very quickly and easily without reading it completely.', 'keyphrase extraction was first done primarily by human annotators, who read the text in detail and then wrote down the most important keyphrases.', 'the disadvantage is that if you work with a lot of documents, this process can take a lot of time.', 'here is where artificial intelligence comes in.', 'currently, classical machine learning methods, that use statistical and linguistic features, are widely used for the extraction process.', 'now with deep learning, it is possible to capture the semantic meaning of a text even better than these classical methods.', 'classical methods look at the frequency, occurrence and order of words in the text, whereas these neural approaches can capture long-term semantic dependencies and context of words in a text.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for result in keyphrases:\n",
        "    if result['entity_group'] == 'KEY':\n",
        "        sentence = sent_tokenize(text)\n",
        "        for token in sentence:\n",
        "            if result['word'] in token:\n",
        "              sentence_str = token.replace(result[\"word\"], \"_____\")\n",
        "              print(sentence_str,\" answer: \",result['word'])\n",
        "        continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Shu3jbfLV8qC",
        "outputId": "29df97b1-a590-4adc-deed-f4bcf82972e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " _____ is a technique in text analysis where you extract the important keyphrases from a document.  answer:  keyphrase extraction\n",
            "_____ was first done primarily by human annotators, who read the text in detail and then wrote down the most important keyphrases.  answer:  keyphrase extraction\n",
            " keyphrase extraction is a technique in _____ where you extract the important keyphrases from a document.  answer:  text analysis\n",
            " _____ is a technique in text analysis where you extract the important keyphrases from a document.  answer:  keyphrase extraction\n",
            "_____ was first done primarily by human annotators, who read the text in detail and then wrote down the most important keyphrases.  answer:  keyphrase extraction\n",
            "here is where _____ comes in.  answer:  artificial intelligence\n",
            "currently, _____ methods, that use statistical and linguistic features, are widely used for the extraction process.  answer:  classical machine learning\n",
            "currently, classical machine learning methods, that use _____ and linguistic features, are widely used for the extraction process.  answer:  statistical\n",
            "currently, classical machine learning methods, that use statistical and _____, are widely used for the extraction process.  answer:  linguistic features\n",
            "now with _____, it is possible to capture the semantic meaning of a text even better than these classical methods.  answer:  deep learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import pandas as pd\n",
        "import random"
      ],
      "metadata": {
        "id": "QadQgsfrqPfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained word2vec model\n",
        "model = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "Pi7y9dskuVzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word=\"statistical\"\n",
        "word=word.split(\" \")"
      ],
      "metadata": {
        "id": "1bCGP4blth1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word=\"run\"\n",
        "similar_words = loaded_model.most_similar(word, topn=20)\n",
        "similar = [w[0] for w in similar_words if w[0][0].lower() !=word[0]]\n",
        "print(similar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXjp_NCivA7_",
        "outputId": "7080374c-3de3-4060-cd54-48afeea46728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['trun', 'srun', 'pre-run', 'go', 'operate', 'test-run', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/fasttext_model\")"
      ],
      "metadata": {
        "id": "Fk5GmiMewxFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load the saved model from disk\n",
        "loaded_model = KeyedVectors.load(\"/content/drive/MyDrive/fasttext_model\")"
      ],
      "metadata": {
        "id": "O6AZYwCIxu2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = loaded_model.most_similar(\"intelligence\")\n",
        "print(similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpWlne7It8rW",
        "outputId": "62aaf38b-0789-4f23-81b3-58b98107b1d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('intelligence-', 0.8000309467315674), ('inteligence', 0.7867905497550964), ('counter-intelligence', 0.7488308548927307), ('non-intelligence', 0.7485793828964233), ('intelligence-related', 0.7421665191650391), ('super-intelligence', 0.7363789081573486), ('Intelligence', 0.7362917065620422), ('intelligence-based', 0.7294193506240845), ('intelligences', 0.7164568305015564), ('intellegence', 0.7142481803894043)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_option(answer):\n",
        "  answer=answer.split(\" \")\n",
        "  options=[]\n",
        "  if(len(answer)>1):\n",
        "    similar_words = loaded_model.most_similar(answer[0], topn=20)\n",
        "    similar = [w[0] for w in similar_words if w[0][0].lower() !=answer[0][0]]\n",
        "    for option in similar:\n",
        "      if len(answer)==2:\n",
        "        options.append(option+\" \"+answer[1])\n",
        "      else:\n",
        "        options.append(option+\" \"+answer[1]+\" \"+answer[2])\n",
        "      \n",
        "  else:\n",
        "    similar_words = loaded_model.most_similar(answer[0], topn=20)\n",
        "    similar = [w[0] for w in similar_words if w[0][0].lower() !=answer[0][0]]\n",
        "    for option in similar:\n",
        "      options.append(option)\n",
        "  return options"
      ],
      "metadata": {
        "id": "xLdlmuK-tgsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "options=generate_option(\"classical machine learning\")\n",
        "print(options)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSakb8exw-Um",
        "outputId": "3146efa8-e622-44bc-8a3f-36a96155fa69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['non-classical machine learning', 'pre-classical machine learning', 'semi-classical machine learning', 'semiclassical machine learning', 'quasi-classical machine learning', 'pseudo-classical machine learning', 'post-classical machine learning', 'preclassical machine learning', 'nonclassical machine learning', 'neo-classical machine learning', 'postclassical machine learning', 'neo-classic machine learning', 'anti-classical machine learning', 'modern machine learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mcq(summary):\n",
        "  keyphrases = extractor(summary.lower())\n",
        "  sentence = sent_tokenize(summary.lower())\n",
        "  for result in keyphrases:\n",
        "    if result['entity_group'] == 'KEY':\n",
        "        for token in sentence:\n",
        "            # if (result['word'] in token and !(result['word'].isalnum())):\n",
        "            if result['word'] in token and not result['word'].isalnum():\n",
        "              sentence_str = token.replace(result[\"word\"], \"_____\")\n",
        "              try:\n",
        "                options=generate_option(result[\"word\"])\n",
        "                random.shuffle(options)\n",
        "              except KeyError:\n",
        "                continue\n",
        "              print(sentence_str)\n",
        "              print(\"Options:\")\n",
        "              for x in range(3):\n",
        "                print(x,\". \",options[x])\n",
        "              print(\"Answer: \",result[\"word\"])\n",
        "              print()\n",
        "        continue"
      ],
      "metadata": {
        "id": "GVZCRR2y8LQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=input()\n",
        "generate_mcq(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B143YUjatG3V",
        "outputId": "9ea10b51-9c3b-4155-dd71-30c959dddc4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The event-driven architecture in Node.js is based on an event loop that listens for incoming events and executes the corresponding event handler functions. In high level, the architecture works as follows:  The Node.js runtime is based on an event-driven, non-blocking I/O model that allows it to handle large amounts of requests and data processing without blocking the main thread.  When a request is received, Node.js creates an event object that represents the request, and adds it to the event queue.  The event loop continuously checks the event queue for new events, and processes them in the order they were added.  When an event is processed, the corresponding event handler function is executed, and any necessary I/O operations are performed asynchronously.  Once the event handler function is completed, the event loop returns to the event queue and continues processing the next event.  Node.js also uses callbacks to handle asynchronous functions and to notify the event loop when they are completed.  In addition to the built-in events and event handlers in Node.js, developers can also create custom events and handlers using the EventEmitter module.  Overall, the event-driven architecture in Node.js allows for highly scalable and efficient applications by utilizing non-blocking I/O and asynchronous processing. By processing events in a loop rather than a traditional thread, Node.js is able to handle large volumes of requests with minimal overhead, making it well-suited for high-performance web applications.\n",
            "the event-driven architecture in node.js is based on an _____ that listens for incoming events and executes the corresponding event handler functions.\n",
            "Options:\n",
            "0 .  incident loop\n",
            "1 .  life-events loop\n",
            "2 .  multi-event loop\n",
            "Answer:  event loop\n",
            "\n",
            "the _____ continuously checks the event queue for new events, and processes them in the order they were added.\n",
            "Options:\n",
            "0 .  multi-event loop\n",
            "1 .  mega-event loop\n",
            "2 .  pre-event loop\n",
            "Answer:  event loop\n",
            "\n",
            "once the event handler function is completed, the _____ returns to the event queue and continues processing the next event.\n",
            "Options:\n",
            "0 .  multi-event loop\n",
            "1 .  incident loop\n",
            "2 .  non-event loop\n",
            "Answer:  event loop\n",
            "\n",
            "node.js also uses callbacks to handle asynchronous functions and to notify the _____ when they are completed.\n",
            "Options:\n",
            "0 .  multi-event loop\n",
            "1 .  non-event loop\n",
            "2 .  seven-event loop\n",
            "Answer:  event loop\n",
            "\n",
            "the event-driven architecture in node.js is based on an event loop that listens for incoming events and executes the corresponding _____.\n",
            "Options:\n",
            "0 .  sub-events handler functions\n",
            "1 .  single-event handler functions\n",
            "2 .  multi-event handler functions\n",
            "Answer:  event handler functions\n",
            "\n",
            "the event-driven architecture in node.js is based on an _____ that listens for incoming events and executes the corresponding event handler functions.\n",
            "Options:\n",
            "0 .  incident loop\n",
            "1 .  multi-event loop\n",
            "2 .  pseudo-event loop\n",
            "Answer:  event loop\n",
            "\n",
            "the _____ continuously checks the event queue for new events, and processes them in the order they were added.\n",
            "Options:\n",
            "0 .  multi-event loop\n",
            "1 .  non-event loop\n",
            "2 .  pre-event loop\n",
            "Answer:  event loop\n",
            "\n",
            "once the event handler function is completed, the _____ returns to the event queue and continues processing the next event.\n",
            "Options:\n",
            "0 .  single-event loop\n",
            "1 .  seven-event loop\n",
            "2 .  pre-event loop\n",
            "Answer:  event loop\n",
            "\n",
            "node.js also uses callbacks to handle asynchronous functions and to notify the _____ when they are completed.\n",
            "Options:\n",
            "0 .  one-event loop\n",
            "1 .  incident loop\n",
            "2 .  non-event loop\n",
            "Answer:  event loop\n",
            "\n",
            "the event-driven architecture in node.js is based on an event loop that listens for incoming events and executes the corresponding _____s.\n",
            "Options:\n",
            "0 .  incident handler function\n",
            "1 .  seven-event handler function\n",
            "2 .  pre-event handler function\n",
            "Answer:  event handler function\n",
            "\n",
            "when an event is processed, the corresponding _____ is executed, and any necessary i/o operations are performed asynchronously.\n",
            "Options:\n",
            "0 .  mega-event handler function\n",
            "1 .  single-event handler function\n",
            "2 .  life-events handler function\n",
            "Answer:  event handler function\n",
            "\n",
            "once the _____ is completed, the event loop returns to the event queue and continues processing the next event.\n",
            "Options:\n",
            "0 .  non-event handler function\n",
            "1 .  mega-event handler function\n",
            "2 .  life-events handler function\n",
            "Answer:  event handler function\n",
            "\n",
            "the event-driven architecture in node.js is based on an _____ that listens for incoming events and executes the corresponding event handler functions.\n",
            "Options:\n",
            "0 .  single-event loop\n",
            "1 .  seven-event loop\n",
            "2 .  sub-events loop\n",
            "Answer:  event loop\n",
            "\n",
            "the _____ continuously checks the event queue for new events, and processes them in the order they were added.\n",
            "Options:\n",
            "0 .  incident loop\n",
            "1 .  pre-event loop\n",
            "2 .  non-events loop\n",
            "Answer:  event loop\n",
            "\n",
            "once the event handler function is completed, the _____ returns to the event queue and continues processing the next event.\n",
            "Options:\n",
            "0 .  single-event loop\n",
            "1 .  mega-event loop\n",
            "2 .  life-events loop\n",
            "Answer:  event loop\n",
            "\n",
            "node.js also uses callbacks to handle asynchronous functions and to notify the _____ when they are completed.\n",
            "Options:\n",
            "0 .  seven-event loop\n",
            "1 .  non-event loop\n",
            "2 .  single-event loop\n",
            "Answer:  event loop\n",
            "\n",
            "when a request is received, node.js creates an event object that represents the request, and adds it to the _____.\n",
            "Options:\n",
            "0 .  incident queue\n",
            "1 .  multi-event queue\n",
            "2 .  life-events queue\n",
            "Answer:  event queue\n",
            "\n",
            "the event loop continuously checks the _____ for new events, and processes them in the order they were added.\n",
            "Options:\n",
            "0 .  sub-events queue\n",
            "1 .  single-event queue\n",
            "2 .  seven-event queue\n",
            "Answer:  event queue\n",
            "\n",
            "once the event handler function is completed, the event loop returns to the _____ and continues processing the next event.\n",
            "Options:\n",
            "0 .  non-events queue\n",
            "1 .  pseudo-event queue\n",
            "2 .  mega-event queue\n",
            "Answer:  event queue\n",
            "\n",
            "node.js also uses callbacks to handle _____ and to notify the event loop when they are completed.\n",
            "Options:\n",
            "0 .  interactive functions\n",
            "1 .  interrupt-driven functions\n",
            "2 .  event-driven functions\n",
            "Answer:  asynchronous functions\n",
            "\n",
            "in addition to the built-in events and _____ in node.js, developers can also create custom events and handlers using the eventemitter module.\n",
            "Options:\n",
            "0 .  life-events handlers\n",
            "1 .  non-events handlers\n",
            "2 .  one-event handlers\n",
            "Answer:  event handlers\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Q4YrLmftNB-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}